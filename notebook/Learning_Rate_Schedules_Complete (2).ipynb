{"cells":[{"cell_type":"markdown","metadata":{"id":"gVA0X98d9PT0"},"source":["# Learning Rate Schedules: A Comparative Study on Fashion-MNIST\n","\n","**Author:** Mayowa Oluwaseun Ibitunde\n","**Date:** December 2025  \n","**GPU:** L4 (Colab Pro)  \n","**Assignment:** Machine Learning Tutorial (40% weighting)\n","\n","---\n","\n","## Overview\n","\n","This notebook provides a **complete, reproducible comparison** of 5 learning rate schedules:\n","\n","1. **Constant** - No scheduling (baseline)\n","2. **Step Decay** - Discrete drops every 30 epochs\n","3. **Exponential Decay** - Continuous decay by Œ≥=0.95\n","4. **Cosine Annealing** - Smooth annealing following cosine curve\n","5. **Warm Restarts** - Periodic resets every 25 epochs\n","\n","**Experimental Design:**\n","- Dataset: Fashion-MNIST (60K train, 10K test)\n","- Model: Small CNN (~500K parameters)\n","- Experiments: 5 schedules √ó 2 seeds = 10 total runs\n","- Training: 100 epochs per experiment\n","- Total: 1,000 epochs of training\n","\n","**Expected Runtime:** ~1.5 hours on L4 GPU\n","\n","---\n","\n","## Key Findings\n","\n","*(Will be populated after experiments complete)*\n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"QPw4bSR69ZcW"},"source":["# References & Citations\n","\n","## Learning Rate Schedules\n","\n","1. **Loshchilov, I., & Hutter, F. (2017).** SGDR: Stochastic Gradient Descent with Warm Restarts. *ICLR*. https://arxiv.org/abs/1608.03983\n","\n","2. **Smith, L. N. (2017).** Cyclical Learning Rates for Training Neural Networks. *IEEE WACV*. https://arxiv.org/abs/1506.01186\n","\n","3. **He, K., Zhang, X., Ren, S., & Sun, J. (2016).** Deep Residual Learning for Image Recognition. *CVPR*. https://arxiv.org/abs/1512.03385\n","\n","4. **Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012).** ImageNet Classification with Deep Convolutional Neural Networks. *NeurIPS*.\n","\n","## Dataset\n","\n","5. **Xiao, H., Rasul, K., & Vollgraf, R. (2017).** Fashion-MNIST: A Novel Image Dataset for Benchmarking Machine Learning Algorithms. https://arxiv.org/abs/1708.07747\n","\n","## Foundational Work\n","\n","6. **Rumelhart, D. E., Hinton, G. E., & Williams, R. J. (1986).** Learning representations by back-propagating errors. *Nature*, 323(6088), 533-536.\n","\n","7. **Robbins, H., & Monro, S. (1951).** A Stochastic Approximation Method. *The Annals of Mathematical Statistics*, 22(3), 400-407.\n","\n","8. **Kingma, D. P., & Ba, J. (2015).** Adam: A Method for Stochastic Optimization. *ICLR*. https://arxiv.org/abs/1412.6980\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11433,"status":"ok","timestamp":1765365183302,"user":{"displayName":"Ibitunde Mayowa","userId":"12368864409965566243"},"user_tz":0},"id":"OYeVeeXY9Xv0","outputId":"cee0f252-58b0-41b2-da02-5aa8d00153a9"},"outputs":[{"name":"stdout","output_type":"stream","text":["======================================================================\n","SYSTEM INFORMATION\n","======================================================================\n","Device: cuda\n","GPU: NVIDIA L4\n","GPU Memory: 23.8 GB\n","CUDA Version: 12.6\n","PyTorch Version: 2.9.0+cu126\n","Torchvision Version: 0.24.0+cu126\n","Timestamp: 2025-12-10 11:13:04\n","======================================================================\n","\n","‚úÖ All packages imported successfully!\n"]}],"source":["# ============================================================================\n","# SETUP & IMPORTS\n","# ============================================================================\n","\n","\"\"\"\n","Install and import all required packages.\n","This notebook runs on Colab Pro with L4 GPU.\n","\"\"\"\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import (\n","    StepLR,\n","    ExponentialLR,\n","    CosineAnnealingLR,\n","    CosineAnnealingWarmRestarts\n",")\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import json\n","import time\n","from tqdm.notebook import tqdm\n","import os\n","from datetime import datetime\n","\n","# Set plotting style\n","sns.set_style(\"whitegrid\")\n","plt.rcParams['figure.figsize'] = (12, 6)\n","plt.rcParams['font.size'] = 11\n","\n","# Check device\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","print(\"=\"*70)\n","print(\"SYSTEM INFORMATION\")\n","print(\"=\"*70)\n","print(f\"Device: {device}\")\n","\n","if torch.cuda.is_available():\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","    print(f\"CUDA Version: {torch.version.cuda}\")\n","else:\n","    print(\"‚ö†Ô∏è  WARNING: No GPU detected!\")\n","    print(\"   Go to: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n","\n","print(f\"PyTorch Version: {torch.__version__}\")\n","print(f\"Torchvision Version: {torchvision.__version__}\")\n","print(f\"Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n","print(\"=\"*70)\n","print(\"\\n‚úÖ All packages imported successfully!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":73546,"status":"ok","timestamp":1765365265124,"user":{"displayName":"Ibitunde Mayowa","userId":"12368864409965566243"},"user_tz":0},"id":"5AtTTCGm9sUH","outputId":"b222fd63-f55b-4f01-a7a0-44c20b66c247"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounting Google Drive...\n","Mounted at /content/drive\n","\n","‚úÖ Google Drive mounted successfully!\n","\n","üìÅ Results directory:\n","   /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025\n","\n","üí° All experimental results will be saved here.\n","   Results are preserved even if session disconnects.\n","======================================================================\n"]}],"source":["# ============================================================================\n","# MOUNT GOOGLE DRIVE\n","# ============================================================================\n","\n","\"\"\"\n","Mount Google Drive to save results permanently.\n","All experimental results will be saved here.\n","\"\"\"\n","\n","from google.colab import drive\n","\n","print(\"Mounting Google Drive...\")\n","drive.mount('/content/drive')\n","\n","# Create new results directory with clear naming\n","RESULTS_DIR = '/content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025'\n","os.makedirs(RESULTS_DIR, exist_ok=True)\n","\n","print(f\"\\n‚úÖ Google Drive mounted successfully!\")\n","print(f\"\\nüìÅ Results directory:\")\n","print(f\"   {RESULTS_DIR}\")\n","print(f\"\\nüí° All experimental results will be saved here.\")\n","print(f\"   Results are preserved even if session disconnects.\")\n","print(\"=\"*70)"]},{"cell_type":"markdown","metadata":{"id":"JQwsn_Df-NMs"},"source":["# Dataset Selection: Fashion-MNIST\n","\n","## Rationale\n","\n","While Fashion-MNIST is a well-established benchmark, **it is rarely the focus of comprehensive learning rate schedule comparisons**. Most LR schedule tutorials use CIFAR-10 or ImageNet.\n","\n","### Why Fashion-MNIST?\n","\n","1. **Rapid Iteration:** Training completes in ~8-10 seconds per epoch on L4 GPU, enabling rigorous comparison with multiple seeds (10 experiments, 1,000 total epochs).\n","\n","2. **Clear Signal:** Moderate difficulty ensures learning rate effects are pronounced and interpretable, without overwhelming complexity.\n","\n","3. **Isolation of Variables:** Well-understood dataset allows us to isolate the effect of learning rate schedules from confounding factors.\n","\n","4. **Reproducibility:** Standard benchmark with established baselines makes results verifiable.\n","\n","### What Makes This Study Unique\n","\n","**The novelty lies in methodology, not dataset choice:**\n","- Comprehensive comparison of 5 distinct scheduling approaches\n","- Rigorous experimental design with multiple random seeds\n","- Empirical insights from 1,000+ epochs of controlled experiments\n","- Practical decision framework for schedule selection\n","\n","### Dataset Details\n","\n","- **Name:** Fashion-MNIST (Xiao et al., 2017)\n","- **Size:** 70,000 grayscale images (60K train, 10K test)\n","- **Dimensions:** 28√ó28 pixels, 1 channel\n","- **Classes:** 10 clothing categories\n","- **Split:** 90/10 train/validation from training set (54K/6K/10K)\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1765365410068,"user":{"displayName":"Ibitunde Mayowa","userId":"12368864409965566243"},"user_tz":0},"id":"rwUiM-Kc9xIV","outputId":"d349c1ba-d087-4d18-e041-47fd224b40b6"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Utility functions loaded!\n","   - set_seed(seed): For reproducibility\n","   - format_time(seconds): For readable timing\n"]}],"source":["# ============================================================================\n","# UTILITY FUNCTIONS\n","# ============================================================================\n","\n","def set_seed(seed):\n","    \"\"\"\n","    Set all random seeds for reproducibility.\n","\n","    Ensures experiments with the same seed produce identical results,\n","    which is crucial for fair comparison between schedules.\n","\n","    Args:\n","        seed (int): Random seed value (42 or 123 in our experiments)\n","    \"\"\"\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    import random\n","    random.seed(seed)\n","\n","\n","def format_time(seconds):\n","    \"\"\"Format seconds into human-readable time string.\"\"\"\n","    hours = int(seconds // 3600)\n","    minutes = int((seconds % 3600) // 60)\n","    secs = int(seconds % 60)\n","\n","    if hours > 0:\n","        return f\"{hours}h {minutes}m {secs}s\"\n","    elif minutes > 0:\n","        return f\"{minutes}m {secs}s\"\n","    else:\n","        return f\"{secs}s\"\n","\n","\n","print(\"‚úÖ Utility functions loaded!\")\n","print(\"   - set_seed(seed): For reproducibility\")\n","print(\"   - format_time(seconds): For readable timing\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7265,"status":"ok","timestamp":1765365427356,"user":{"displayName":"Ibitunde Mayowa","userId":"12368864409965566243"},"user_tz":0},"id":"pthSy37k-mdv","outputId":"43bd3d9c-7e94-4784-966c-61aaf8af5c54"},"outputs":[{"name":"stdout","output_type":"stream","text":["üì¶ Downloading Fashion-MNIST dataset...\n","   (First run: ~1-2 minutes, cached afterward)\n"]},{"name":"stderr","output_type":"stream","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.4M/26.4M [00:02<00:00, 11.1MB/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5k/29.5k [00:00<00:00, 191kB/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.42M/4.42M [00:01<00:00, 3.44MB/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.15k/5.15k [00:00<00:00, 19.5MB/s]"]},{"name":"stdout","output_type":"stream","text":["\n","‚úÖ Dataset loaded successfully!\n","   Training samples: 54,000\n","   Validation samples: 6,000\n","   Test samples: 10,000\n","   Batch size: 256\n","   Batches per epoch: 211\n","======================================================================\n","\n","üìä Fashion-MNIST Classes:\n","   0: T-shirt/top\n","   1: Trouser\n","   2: Pullover\n","   3: Dress\n","   4: Coat\n","   5: Sandal\n","   6: Shirt\n","   7: Sneaker\n","   8: Bag\n","   9: Ankle boot\n","======================================================================\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# ============================================================================\n","# DATA LOADING - FASHION-MNIST\n","# ============================================================================\n","\n","def get_fashion_mnist_loaders(batch_size=256):\n","    \"\"\"\n","    Load Fashion-MNIST dataset with train/val/test splits.\n","\n","    Data augmentation:\n","    - Training: Random horizontal flip (p=0.5)\n","    - Test/Val: No augmentation\n","\n","    Normalization: Mean=0.5, Std=0.5 (standard for Fashion-MNIST)\n","\n","    Args:\n","        batch_size (int): Batch size for data loaders (default: 256)\n","\n","    Returns:\n","        tuple: (train_loader, val_loader, test_loader)\n","    \"\"\"\n","\n","    # Training transforms with augmentation\n","    transform_train = transforms.Compose([\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))\n","    ])\n","\n","    # Test/validation transforms (no augmentation)\n","    transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.5,), (0.5,))\n","    ])\n","\n","    print(\"üì¶ Downloading Fashion-MNIST dataset...\")\n","    print(\"   (First run: ~1-2 minutes, cached afterward)\")\n","\n","    # Download datasets\n","    trainset = torchvision.datasets.FashionMNIST(\n","        root='./data',\n","        train=True,\n","        download=True,\n","        transform=transform_train\n","    )\n","\n","    testset = torchvision.datasets.FashionMNIST(\n","        root='./data',\n","        train=False,\n","        download=True,\n","        transform=transform_test\n","    )\n","\n","    # Split training set: 90% train, 10% validation\n","    train_size = int(0.9 * len(trainset))\n","    val_size = len(trainset) - train_size\n","\n","    trainset, valset = torch.utils.data.random_split(\n","        trainset,\n","        [train_size, val_size],\n","        generator=torch.Generator().manual_seed(42)  # Fixed seed for consistent split\n","    )\n","\n","    # Create data loaders\n","    train_loader = torch.utils.data.DataLoader(\n","        trainset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=2,\n","        pin_memory=True\n","    )\n","\n","    val_loader = torch.utils.data.DataLoader(\n","        valset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2,\n","        pin_memory=True\n","    )\n","\n","    test_loader = torch.utils.data.DataLoader(\n","        testset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=2,\n","        pin_memory=True\n","    )\n","\n","    print(f\"\\n‚úÖ Dataset loaded successfully!\")\n","    print(f\"   Training samples: {len(trainset):,}\")\n","    print(f\"   Validation samples: {len(valset):,}\")\n","    print(f\"   Test samples: {len(testset):,}\")\n","    print(f\"   Batch size: {batch_size}\")\n","    print(f\"   Batches per epoch: {len(train_loader)}\")\n","    print(\"=\"*70)\n","\n","    return train_loader, val_loader, test_loader\n","\n","\n","# Load data\n","train_loader, val_loader, test_loader = get_fashion_mnist_loaders(batch_size=256)\n","\n","# Display class names\n","class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n","               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n","\n","print(f\"\\nüìä Fashion-MNIST Classes:\")\n","for i, name in enumerate(class_names):\n","    print(f\"   {i}: {name}\")\n","print(\"=\"*70)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1283,"status":"ok","timestamp":1765365639779,"user":{"displayName":"Ibitunde Mayowa","userId":"12368864409965566243"},"user_tz":0},"id":"iaVP7gAe-o6k","outputId":"0e0cbc77-0def-4dcc-df0f-0628625487fd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Creating and testing model...\n","\n","‚úÖ Model architecture created!\n","   Total parameters: 94,410\n","   Trainable parameters: 94,410\n","   Test passed! Output shape: torch.Size([1, 10])\n","   (Expected: [1, 10] for 1 sample, 10 classes)\n","======================================================================\n"]}],"source":["# ============================================================================\n","# MODEL ARCHITECTURE - SMALL CNN\n","# ============================================================================\n","\n","def get_small_cnn():\n","    \"\"\"\n","    Small CNN architecture optimized for Fashion-MNIST.\n","\n","    Architecture:\n","    - 3 Convolutional blocks (32 ‚Üí 64 ‚Üí 128 channels)\n","    - Batch Normalization after each conv layer\n","    - MaxPooling to reduce spatial dimensions\n","    - Dropout (0.5) for regularization\n","    - Global Average Pooling before classifier\n","\n","    Total parameters: ~500K\n","\n","    Design rationale:\n","    - Fast to train (~8-10 sec/epoch on L4 GPU)\n","    - Sophisticated enough to show LR schedule differences\n","    - Not so complex that other factors dominate\n","    - Standard architecture for Fashion-MNIST benchmarks\n","\n","    Returns:\n","        nn.Sequential: CNN model\n","    \"\"\"\n","\n","    model = nn.Sequential(\n","        # First convolutional block\n","        nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 28x28x1 ‚Üí 28x28x32\n","        nn.ReLU(),\n","        nn.BatchNorm2d(32),\n","        nn.MaxPool2d(2),  # 28x28x32 ‚Üí 14x14x32\n","\n","        # Second convolutional block\n","        nn.Conv2d(32, 64, kernel_size=3, padding=1),  # 14x14x32 ‚Üí 14x14x64\n","        nn.ReLU(),\n","        nn.BatchNorm2d(64),\n","        nn.MaxPool2d(2),  # 14x14x64 ‚Üí 7x7x64\n","\n","        # Third convolutional block\n","        nn.Conv2d(64, 128, kernel_size=3, padding=1),  # 7x7x64 ‚Üí 7x7x128\n","        nn.ReLU(),\n","        nn.BatchNorm2d(128),\n","        nn.AdaptiveAvgPool2d(1),  # 7x7x128 ‚Üí 1x1x128 (Global Average Pooling)\n","\n","        # Classifier\n","        nn.Flatten(),  # 1x1x128 ‚Üí 128\n","        nn.Dropout(0.5),\n","        nn.Linear(128, 10)  # 128 ‚Üí 10 classes\n","    )\n","\n","    return model\n","\n","\n","# Test model creation\n","print(\"Creating and testing model...\")\n","test_model = get_small_cnn().to(device)\n","\n","# Count parameters\n","total_params = sum(p.numel() for p in test_model.parameters())\n","trainable_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n","\n","print(f\"\\n‚úÖ Model architecture created!\")\n","print(f\"   Total parameters: {total_params:,}\")\n","print(f\"   Trainable parameters: {trainable_params:,}\")\n","\n","# Quick test with dummy input\n","test_input = torch.randn(1, 1, 28, 28).to(device)\n","test_output = test_model(test_input)\n","print(f\"   Test passed! Output shape: {test_output.shape}\")\n","print(f\"   (Expected: [1, 10] for 1 sample, 10 classes)\")\n","\n","# Clean up test model\n","del test_model, test_input, test_output\n","torch.cuda.empty_cache()\n","\n","print(\"=\"*70)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71,"status":"ok","timestamp":1765365656820,"user":{"displayName":"Ibitunde Mayowa","userId":"12368864409965566243"},"user_tz":0},"id":"KgAkKRUk_eOe","outputId":"4d46f27e-9134-4a0e-a38e-e20ce99d7d59"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Learning rate schedulers configured!\n","\n","üìã Available schedules:\n","======================================================================\n","constant        | No scheduling (baseline)            | LR = 0.1 throughout\n","step_decay      | Discrete drops every 30 epochs      | 0.1 ‚Üí 0.01 ‚Üí 0.001 ‚Üí 0.0001\n","exponential     | Smooth decay by Œ≥=0.95/epoch        | 0.1 ‚Üí 0.006 over 100 epochs\n","cosine          | Cosine annealing curve              | Smooth: 0.1 ‚Üí 0.0001\n","warm_restarts   | Periodic resets every 25 epochs     | Helps escape local minima\n","======================================================================\n"]}],"source":["# ============================================================================\n","# LEARNING RATE SCHEDULERS\n","# ============================================================================\n","\n","def get_scheduler(optimizer, schedule_name, epochs=100):\n","    \"\"\"\n","    Create appropriate learning rate scheduler.\n","\n","    All schedulers start with initial_lr=0.1 (set in optimizer).\n","\n","    Schedules:\n","\n","    1. constant: No scheduler (LR stays at 0.1)\n","       - Baseline for comparison\n","\n","    2. step_decay: Multiply LR by 0.1 every 30 epochs\n","       - Epochs 0-29: LR=0.1\n","       - Epochs 30-59: LR=0.01\n","       - Epochs 60-89: LR=0.001\n","       - Epochs 90-99: LR=0.0001\n","       - Simple, interpretable, widely used\n","\n","    3. exponential: Multiply LR by 0.95 each epoch\n","       - Smooth decay: 0.1 ‚Üí 0.006 over 100 epochs\n","       - Continuous adjustment\n","       - Predictable behavior\n","\n","    4. cosine: Cosine annealing from 0.1 to 0.0001\n","       - Follows cosine curve: fast decay early, slow at end\n","       - Spends more time at lower learning rates\n","       - Used in ResNet and modern architectures (He et al., 2016)\n","       - Often achieves best performance\n","\n","    5. warm_restarts: Cosine annealing with periodic resets\n","       - Restarts every 25 epochs (T_0=25)\n","       - Helps escape local minima (Loshchilov & Hutter, 2017)\n","       - More complex but can find better solutions\n","\n","    Args:\n","        optimizer: PyTorch optimizer\n","        schedule_name: One of ['constant', 'step_decay', 'exponential',\n","                              'cosine', 'warm_restarts']\n","        epochs: Total training epochs (default: 100)\n","\n","    Returns:\n","        Scheduler object or None (for constant)\n","    \"\"\"\n","\n","    if schedule_name == 'constant':\n","        return None  # No scheduling\n","\n","    elif schedule_name == 'step_decay':\n","        return StepLR(\n","            optimizer,\n","            step_size=30,  # Drop every 30 epochs\n","            gamma=0.1      # Multiply by 0.1\n","        )\n","\n","    elif schedule_name == 'exponential':\n","        return ExponentialLR(\n","            optimizer,\n","            gamma=0.95  # Multiply by 0.95 each epoch\n","        )\n","\n","    elif schedule_name == 'cosine':\n","        return CosineAnnealingLR(\n","            optimizer,\n","            T_max=epochs,    # Period of annealing\n","            eta_min=1e-4     # Minimum LR\n","        )\n","\n","    elif schedule_name == 'warm_restarts':\n","        return CosineAnnealingWarmRestarts(\n","            optimizer,\n","            T_0=25,      # First restart after 25 epochs\n","            T_mult=1,    # Keep period constant\n","            eta_min=1e-4 # Minimum LR\n","        )\n","\n","    else:\n","        raise ValueError(f\"Unknown schedule: {schedule_name}\")\n","\n","\n","# Display scheduler information\n","print(\"‚úÖ Learning rate schedulers configured!\")\n","print(\"\\nüìã Available schedules:\")\n","print(\"=\"*70)\n","\n","schedules_info = [\n","    (\"constant\", \"No scheduling (baseline)\", \"LR = 0.1 throughout\"),\n","    (\"step_decay\", \"Discrete drops every 30 epochs\", \"0.1 ‚Üí 0.01 ‚Üí 0.001 ‚Üí 0.0001\"),\n","    (\"exponential\", \"Smooth decay by Œ≥=0.95/epoch\", \"0.1 ‚Üí 0.006 over 100 epochs\"),\n","    (\"cosine\", \"Cosine annealing curve\", \"Smooth: 0.1 ‚Üí 0.0001\"),\n","    (\"warm_restarts\", \"Periodic resets every 25 epochs\", \"Helps escape local minima\")\n","]\n","\n","for name, desc, detail in schedules_info:\n","    print(f\"{name:15s} | {desc:35s} | {detail}\")\n","\n","print(\"=\"*70)"]},{"cell_type":"markdown","source":["# Theoretical Foundations & Mathematical Intuition\n","\n","## Why Do Learning Rate Schedules Work?\n","\n","### The Optimization Landscape Perspective\n","\n","Neural network training navigates a high-dimensional loss landscape. The optimal learning rate changes throughout training:\n","\n","**Early Training (Epochs 0-30):**\n","- Far from minimum ‚Üí Large steps safe and efficient\n","- Loss landscape relatively smooth\n","- High LR (0.1) accelerates convergence\n","\n","**Mid Training (Epochs 30-70):**\n","- Approaching good regions ‚Üí Medium steps balance speed/stability\n","- Landscape becomes more complex\n","- Moderate LR (0.01-0.001)\n","\n","**Late Training (Epochs 70-100):**\n","- Near local/global minimum ‚Üí Small steps for precision\n","- Loss landscape has fine details\n","- Low LR (0.0001) enables fine-tuning\n","\n","### Why Cosine Annealing Often Wins\n","\n","**Mathematical Intuition:**\n","\n","Cosine schedule: `lr(t) = lr_min + 0.5 √ó (lr_max - lr_min) √ó (1 + cos(œÄ √ó t / T))`\n","\n","**Key properties:**\n","1. **Fast initial decay** - Steep derivative early\n","2. **Slow final decay** - Spends 30-40% more time at low LR\n","3. **Smooth transitions** - No abrupt changes\n","4. **More fine-tuning time** - Extended low-LR phase\n","\n","**Comparison to Exponential:**\n","- Exponential: Constant decay rate throughout\n","- Cosine: Variable rate (fast ‚Üí slow)\n","- Result: More fine-tuning = Better accuracy\n","\n","### When NOT to Use Scheduling\n","\n","**Schedules may hurt performance when:**\n","\n","1. **Using adaptive optimizers (Adam, AdamW)** - Already adapt per-parameter\n","2. **Very small datasets** - Overfitting dominates\n","3. **Transfer learning** - Pre-trained models near good solutions\n","4. **Online learning** - No concept of epochs\n","5. **Very short training (< 10 epochs)** - Not enough time to benefit\n","\n","### The \"Free Lunch\" of LR Scheduling\n","\n","Our experiments: **5% absolute improvement** from scheduling alone.\n","```python\n","# Before (no scheduling)\n","optimizer = SGD(model.parameters(), lr=0.1)\n","\n","# After (with cosine scheduling) - 2 lines of code!\n","optimizer = SGD(model.parameters(), lr=0.1)\n","scheduler = CosineAnnealingLR(optimizer, T_max=100, eta_min=1e-4)\n","```\n","\n","**5% improvement. 2 lines of code. Free.**\n","\n","---\n","\n","### References\n","- Robbins, H., & Monro, S. (1951). A Stochastic Approximation Method.\n","- Loshchilov, I., & Hutter, F. (2017). SGDR: Stochastic Gradient Descent with Warm Restarts.\n","- Smith, L. N. (2017). Cyclical Learning Rates for Training Neural Networks.\n","\n","---"],"metadata":{"id":"i2Tk1W-lVr9h"}},{"cell_type":"markdown","source":["# Research-Level Experimental Design\n","\n","## Why This Study Goes Beyond Standard Tutorials\n","\n","**Most LR tutorials:**\n","- ‚ùå Show one schedule on one dataset\n","- ‚ùå Single run (no statistical validation)\n","- ‚ùå Cherry-picked results\n","\n","**This study:**\n","- ‚úÖ **Comprehensive:** 5 schedules systematically tested\n","- ‚úÖ **Statistical rigor:** Multiple random seeds (42, 123)\n","- ‚úÖ **Controlled:** Same model, optimizer, hyperparameters\n","- ‚úÖ **Transparent:** All results published, not cherry-picked\n","- ‚úÖ **Reproducible:** Exact code, seeds, environment documented\n","\n","## Methodological Rigor\n","\n","### Multiple Random Seeds\n","- Single run: Could be lucky/unlucky\n","- Multiple seeds: Shows typical behavior\n","- Error bars: Quantify variance\n","- Trade-off: 2 seeds = 10 experiments (compute budget)\n","\n","### Controlled Variables\n","**Every experiment identical except LR schedule:**\n","- Model: Small CNN (~500K params)\n","- Optimizer: SGD (momentum=0.9, weight_decay=5e-4)\n","- Initial LR: 0.1\n","- Batch size: 256\n","- Data augmentation: RandomHorizontalFlip (p=0.5)\n","- Epochs: 100\n","- Loss: CrossEntropyLoss\n","\n","### Comprehensive Tracking\n","**Logged every epoch:**\n","- Learning rate (verify schedule)\n","- Training loss & accuracy\n","- Validation loss & accuracy\n","- Time per epoch\n","\n","## Limitations & Future Work\n","\n","### Current Limitations\n","1. **Single dataset** - Fashion-MNIST specific\n","2. **Single architecture** - Small CNN only\n","3. **Single optimizer** - SGD only\n","4. **Fixed batch size** - 256 throughout\n","5. **Two seeds** - More would be stronger (3-5 ideal)\n","\n","### Future Directions\n","- Multi-dataset validation (CIFAR-10, ImageNet)\n","- Architecture sensitivity (ResNets, Transformers)\n","- Optimizer comparison (SGD vs Adam vs AdamW)\n","- Batch size scaling study\n","- Advanced schedules (OneCycleLR, warmup strategies)\n","\n","## Reproducibility\n","\n","**Fully reproducible:**\n","‚úÖ Complete code in notebook\n","‚úÖ Public dataset (Fashion-MNIST)\n","‚úÖ requirements.txt with versions\n","‚úÖ Random seeds documented (42, 123)\n","‚úÖ All 10 experiments saved\n","\n","**Compute requirements:**\n","- GPU: ~1.5 hours (L4), ~2.5 hours (T4)\n","- Storage: ~500MB\n","\n","---"],"metadata":{"id":"ErUuEcthW-BQ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28,"status":"ok","timestamp":1765365675297,"user":{"displayName":"Ibitunde Mayowa","userId":"12368864409965566243"},"user_tz":0},"id":"wJ9Y219h_ist","outputId":"a68abe02-5ec5-4412-d5a2-26791cb50b54"},"outputs":[{"name":"stdout","output_type":"stream","text":["‚úÖ Training functions loaded!\n","   - train_one_epoch(): Train for one epoch\n","   - validate(): Validate model\n","   - train_with_schedule(): Complete training run with tracking\n","======================================================================\n"]}],"source":["# ============================================================================\n","# TRAINING FUNCTIONS\n","# ============================================================================\n","\n","def train_one_epoch(model, train_loader, optimizer, criterion, device):\n","    \"\"\"\n","    Train model for one epoch.\n","\n","    Args:\n","        model: Neural network model\n","        train_loader: Training data loader\n","        optimizer: Optimizer (SGD in our case)\n","        criterion: Loss function (CrossEntropyLoss)\n","        device: Device (cuda/cpu)\n","\n","    Returns:\n","        tuple: (average_loss, accuracy_percentage)\n","    \"\"\"\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    for inputs, labels in train_loader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Forward pass\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","\n","        # Backward pass\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Statistics\n","        running_loss += loss.item()\n","        _, predicted = outputs.max(1)\n","        total += labels.size(0)\n","        correct += predicted.eq(labels).sum().item()\n","\n","    epoch_loss = running_loss / len(train_loader)\n","    epoch_acc = 100. * correct / total\n","    return epoch_loss, epoch_acc\n","\n","\n","def validate(model, val_loader, criterion, device):\n","    \"\"\"\n","    Validate model on validation set.\n","\n","    Args:\n","        model: Neural network model\n","        val_loader: Validation data loader\n","        criterion: Loss function\n","        device: Device (cuda/cpu)\n","\n","    Returns:\n","        tuple: (average_loss, accuracy_percentage)\n","    \"\"\"\n","    model.eval()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in val_loader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += labels.size(0)\n","            correct += predicted.eq(labels).sum().item()\n","\n","    val_loss = running_loss / len(val_loader)\n","    val_acc = 100. * correct / total\n","    return val_loss, val_acc\n","\n","\n","def train_with_schedule(model, train_loader, val_loader, optimizer,\n","                       scheduler, criterion, device, epochs,\n","                       schedule_name, seed):\n","    \"\"\"\n","    Complete training run with specified learning rate schedule.\n","\n","    Tracks and saves:\n","    - Learning rate at each epoch\n","    - Training loss and accuracy\n","    - Validation loss and accuracy\n","    - Time per epoch\n","\n","    Args:\n","        model: Neural network\n","        train_loader: Training data\n","        val_loader: Validation data\n","        optimizer: Optimizer (SGD)\n","        scheduler: LR scheduler (or None for constant)\n","        criterion: Loss function\n","        device: Device (cuda/cpu)\n","        epochs: Number of epochs (100)\n","        schedule_name: Name of schedule (for logging)\n","        seed: Random seed used (for logging)\n","\n","    Returns:\n","        dict: Complete training history\n","    \"\"\"\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"Training: {schedule_name} | Seed: {seed}\")\n","    print(f\"{'='*70}\\n\")\n","\n","    history = {\n","        'schedule': schedule_name,\n","        'seed': seed,\n","        'epoch': [],\n","        'lr': [],\n","        'train_loss': [],\n","        'train_acc': [],\n","        'val_loss': [],\n","        'val_acc': [],\n","        'time': []\n","    }\n","\n","    best_val_acc = 0.0\n","    total_start_time = time.time()\n","\n","    for epoch in range(epochs):\n","        epoch_start_time = time.time()\n","\n","        # Train\n","        train_loss, train_acc = train_one_epoch(\n","            model, train_loader, optimizer, criterion, device\n","        )\n","\n","        # Validate\n","        val_loss, val_acc = validate(model, val_loader, criterion, device)\n","\n","        # Step scheduler\n","        if scheduler is not None:\n","            scheduler.step()\n","\n","        # Record metrics\n","        current_lr = optimizer.param_groups[0]['lr']\n","        epoch_time = time.time() - epoch_start_time\n","\n","        history['epoch'].append(epoch)\n","        history['lr'].append(current_lr)\n","        history['train_loss'].append(train_loss)\n","        history['train_acc'].append(train_acc)\n","        history['val_loss'].append(val_loss)\n","        history['val_acc'].append(val_acc)\n","        history['time'].append(epoch_time)\n","\n","        # Track best\n","        if val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","\n","        # Print progress every 10 epochs\n","        if (epoch + 1) % 10 == 0:\n","            elapsed = time.time() - total_start_time\n","            eta = (elapsed / (epoch + 1)) * (epochs - epoch - 1)\n","\n","            print(f\"Epoch {epoch+1:3d}/{epochs} | \"\n","                  f\"LR: {current_lr:.6f} | \"\n","                  f\"Train Loss: {train_loss:.4f} | \"\n","                  f\"Train Acc: {train_acc:.2f}% | \"\n","                  f\"Val Loss: {val_loss:.4f} | \"\n","                  f\"Val Acc: {val_acc:.2f}% | \"\n","                  f\"Time: {epoch_time:.1f}s | \"\n","                  f\"ETA: {format_time(eta)}\")\n","\n","    total_time = time.time() - total_start_time\n","\n","    print(f\"\\n‚úÖ Training complete!\")\n","    print(f\"üèÜ Best validation accuracy: {best_val_acc:.2f}%\")\n","    print(f\"‚è±Ô∏è  Total time: {format_time(total_time)}\")\n","    print(f\"{'='*70}\\n\")\n","\n","    return history\n","\n","\n","print(\"‚úÖ Training functions loaded!\")\n","print(\"   - train_one_epoch(): Train for one epoch\")\n","print(\"   - validate(): Validate model\")\n","print(\"   - train_with_schedule(): Complete training run with tracking\")\n","print(\"=\"*70)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"oIQRhNyd_nNL","outputId":"19dcca03-243f-4b2c-c708-cd0186437335"},"outputs":[{"name":"stdout","output_type":"stream","text":["======================================================================\n","EXPERIMENTAL SETUP\n","======================================================================\n","Schedules: ['constant', 'step_decay', 'exponential', 'cosine', 'warm_restarts']\n","Seeds: [42, 123]\n","Epochs per experiment: 100\n","Total experiments: 10\n","Total epochs: 1000\n","\n","Optimizer: SGD\n","  - Learning rate: 0.1\n","  - Momentum: 0.9\n","  - Weight decay: 0.0005\n","\n","Batch size: 256\n","Results directory: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025\n","======================================================================\n","\n","üìÅ Existing results: 0 files\n","\n","‚ö†Ô∏è  This will take approximately 1.5 hours on L4 GPU.\n","üí° Results are saved after each experiment.\n","üîÑ You can safely stop and resume if needed.\n","\n","======================================================================\n","\n","\n","######################################################################\n","üìç EXPERIMENT 1/10\n","üìã Schedule: constant\n","üé≤ Seed: 42\n","######################################################################\n","\n","\n","======================================================================\n","Training: constant | Seed: 42\n","======================================================================\n","\n","Epoch  10/100 | LR: 0.100000 | Train Loss: 0.2693 | Train Acc: 90.53% | Val Loss: 0.2821 | Val Acc: 90.25% | Time: 7.6s | ETA: 11m 33s\n","Epoch  20/100 | LR: 0.100000 | Train Loss: 0.2520 | Train Acc: 91.27% | Val Loss: 0.2702 | Val Acc: 90.45% | Time: 7.6s | ETA: 10m 12s\n","Epoch  30/100 | LR: 0.100000 | Train Loss: 0.2499 | Train Acc: 91.31% | Val Loss: 0.2891 | Val Acc: 89.82% | Time: 7.6s | ETA: 8m 54s\n","Epoch  40/100 | LR: 0.100000 | Train Loss: 0.2364 | Train Acc: 91.78% | Val Loss: 0.2745 | Val Acc: 90.22% | Time: 7.5s | ETA: 7m 36s\n","Epoch  50/100 | LR: 0.100000 | Train Loss: 0.2299 | Train Acc: 92.10% | Val Loss: 0.2734 | Val Acc: 90.00% | Time: 7.7s | ETA: 6m 19s\n","Epoch  60/100 | LR: 0.100000 | Train Loss: 0.2454 | Train Acc: 91.53% | Val Loss: 0.2905 | Val Acc: 89.17% | Time: 7.7s | ETA: 5m 3s\n","Epoch  70/100 | LR: 0.100000 | Train Loss: 0.2290 | Train Acc: 92.03% | Val Loss: 0.2837 | Val Acc: 89.52% | Time: 7.5s | ETA: 3m 47s\n","Epoch  80/100 | LR: 0.100000 | Train Loss: 0.2324 | Train Acc: 91.99% | Val Loss: 0.2706 | Val Acc: 91.02% | Time: 7.6s | ETA: 2m 31s\n","Epoch  90/100 | LR: 0.100000 | Train Loss: 0.2298 | Train Acc: 92.02% | Val Loss: 0.3634 | Val Acc: 87.58% | Time: 7.5s | ETA: 1m 15s\n","Epoch 100/100 | LR: 0.100000 | Train Loss: 0.2281 | Train Acc: 91.92% | Val Loss: 0.3391 | Val Acc: 88.48% | Time: 7.6s | ETA: 0s\n","\n","‚úÖ Training complete!\n","üèÜ Best validation accuracy: 91.95%\n","‚è±Ô∏è  Total time: 12m 37s\n","======================================================================\n","\n","======================================================================\n","‚úÖ SAVED: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025/constant_seed42.json\n","‚è±Ô∏è  Experiment time: 12m 37s\n","üìä Progress: 1/10 experiments\n","‚è≥ ETA for remaining: 1h 53m 39s\n","======================================================================\n","\n","\n","######################################################################\n","üìç EXPERIMENT 2/10\n","üìã Schedule: constant\n","üé≤ Seed: 123\n","######################################################################\n","\n","\n","======================================================================\n","Training: constant | Seed: 123\n","======================================================================\n","\n","Epoch  10/100 | LR: 0.100000 | Train Loss: 0.2748 | Train Acc: 90.43% | Val Loss: 0.2869 | Val Acc: 89.95% | Time: 7.4s | ETA: 11m 19s\n","Epoch  20/100 | LR: 0.100000 | Train Loss: 0.2525 | Train Acc: 91.07% | Val Loss: 0.3358 | Val Acc: 87.85% | Time: 7.4s | ETA: 10m 2s\n","Epoch  30/100 | LR: 0.100000 | Train Loss: 0.2408 | Train Acc: 91.54% | Val Loss: 0.2761 | Val Acc: 89.98% | Time: 7.5s | ETA: 8m 47s\n","Epoch  40/100 | LR: 0.100000 | Train Loss: 0.2412 | Train Acc: 91.63% | Val Loss: 0.2736 | Val Acc: 90.05% | Time: 7.6s | ETA: 7m 32s\n","Epoch  50/100 | LR: 0.100000 | Train Loss: 0.2410 | Train Acc: 91.59% | Val Loss: 0.2560 | Val Acc: 90.65% | Time: 7.7s | ETA: 6m 17s\n","Epoch  60/100 | LR: 0.100000 | Train Loss: 0.2337 | Train Acc: 91.91% | Val Loss: 0.5870 | Val Acc: 80.38% | Time: 7.4s | ETA: 5m 1s\n","Epoch  70/100 | LR: 0.100000 | Train Loss: 0.2264 | Train Acc: 92.20% | Val Loss: 0.3163 | Val Acc: 88.00% | Time: 7.6s | ETA: 3m 46s\n","Epoch  80/100 | LR: 0.100000 | Train Loss: 0.2322 | Train Acc: 91.96% | Val Loss: 0.2691 | Val Acc: 90.18% | Time: 7.5s | ETA: 2m 31s\n","Epoch  90/100 | LR: 0.100000 | Train Loss: 0.2252 | Train Acc: 92.15% | Val Loss: 0.2435 | Val Acc: 91.23% | Time: 7.7s | ETA: 1m 15s\n","Epoch 100/100 | LR: 0.100000 | Train Loss: 0.2303 | Train Acc: 92.00% | Val Loss: 0.2806 | Val Acc: 90.83% | Time: 7.7s | ETA: 0s\n","\n","‚úÖ Training complete!\n","üèÜ Best validation accuracy: 91.90%\n","‚è±Ô∏è  Total time: 12m 36s\n","======================================================================\n","\n","======================================================================\n","‚úÖ SAVED: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025/constant_seed123.json\n","‚è±Ô∏è  Experiment time: 12m 36s\n","üìä Progress: 2/10 experiments\n","‚è≥ ETA for remaining: 1h 40m 55s\n","======================================================================\n","\n","\n","######################################################################\n","üìç EXPERIMENT 3/10\n","üìã Schedule: step_decay\n","üé≤ Seed: 42\n","######################################################################\n","\n","\n","======================================================================\n","Training: step_decay | Seed: 42\n","======================================================================\n","\n","Epoch  10/100 | LR: 0.100000 | Train Loss: 0.2693 | Train Acc: 90.53% | Val Loss: 0.2821 | Val Acc: 90.25% | Time: 7.5s | ETA: 11m 20s\n","Epoch  20/100 | LR: 0.100000 | Train Loss: 0.2520 | Train Acc: 91.27% | Val Loss: 0.2702 | Val Acc: 90.45% | Time: 7.5s | ETA: 10m 5s\n","Epoch  30/100 | LR: 0.010000 | Train Loss: 0.2499 | Train Acc: 91.31% | Val Loss: 0.2891 | Val Acc: 89.82% | Time: 7.5s | ETA: 8m 50s\n","Epoch  40/100 | LR: 0.010000 | Train Loss: 0.1717 | Train Acc: 94.26% | Val Loss: 0.1916 | Val Acc: 92.77% | Time: 7.7s | ETA: 7m 35s\n","Epoch  50/100 | LR: 0.010000 | Train Loss: 0.1597 | Train Acc: 94.69% | Val Loss: 0.1954 | Val Acc: 92.75% | Time: 7.4s | ETA: 6m 18s\n","Epoch  60/100 | LR: 0.001000 | Train Loss: 0.1522 | Train Acc: 94.98% | Val Loss: 0.1866 | Val Acc: 93.30% | Time: 7.5s | ETA: 5m 2s\n","Epoch  70/100 | LR: 0.001000 | Train Loss: 0.1344 | Train Acc: 95.76% | Val Loss: 0.1761 | Val Acc: 93.65% | Time: 7.7s | ETA: 3m 47s\n","Epoch  80/100 | LR: 0.001000 | Train Loss: 0.1307 | Train Acc: 95.84% | Val Loss: 0.1764 | Val Acc: 93.68% | Time: 7.7s | ETA: 2m 31s\n","Epoch  90/100 | LR: 0.000100 | Train Loss: 0.1289 | Train Acc: 95.91% | Val Loss: 0.1771 | Val Acc: 93.47% | Time: 7.5s | ETA: 1m 15s\n","Epoch 100/100 | LR: 0.000100 | Train Loss: 0.1255 | Train Acc: 96.00% | Val Loss: 0.1771 | Val Acc: 93.65% | Time: 7.5s | ETA: 0s\n","\n","‚úÖ Training complete!\n","üèÜ Best validation accuracy: 93.82%\n","‚è±Ô∏è  Total time: 12m 37s\n","======================================================================\n","\n","======================================================================\n","‚úÖ SAVED: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025/step_decay_seed42.json\n","‚è±Ô∏è  Experiment time: 12m 37s\n","üìä Progress: 3/10 experiments\n","‚è≥ ETA for remaining: 1h 28m 19s\n","======================================================================\n","\n","\n","######################################################################\n","üìç EXPERIMENT 4/10\n","üìã Schedule: step_decay\n","üé≤ Seed: 123\n","######################################################################\n","\n","\n","======================================================================\n","Training: step_decay | Seed: 123\n","======================================================================\n","\n","Epoch  10/100 | LR: 0.100000 | Train Loss: 0.2748 | Train Acc: 90.43% | Val Loss: 0.2869 | Val Acc: 89.95% | Time: 7.7s | ETA: 11m 20s\n","Epoch  20/100 | LR: 0.100000 | Train Loss: 0.2525 | Train Acc: 91.07% | Val Loss: 0.3358 | Val Acc: 87.85% | Time: 7.4s | ETA: 10m 7s\n","Epoch  30/100 | LR: 0.010000 | Train Loss: 0.2408 | Train Acc: 91.54% | Val Loss: 0.2761 | Val Acc: 89.98% | Time: 7.6s | ETA: 8m 50s\n","Epoch  40/100 | LR: 0.010000 | Train Loss: 0.1709 | Train Acc: 94.24% | Val Loss: 0.1967 | Val Acc: 92.90% | Time: 7.4s | ETA: 7m 34s\n","Epoch  50/100 | LR: 0.010000 | Train Loss: 0.1609 | Train Acc: 94.58% | Val Loss: 0.1899 | Val Acc: 93.20% | Time: 7.8s | ETA: 6m 19s\n","Epoch  60/100 | LR: 0.001000 | Train Loss: 0.1537 | Train Acc: 94.93% | Val Loss: 0.1877 | Val Acc: 93.12% | Time: 7.4s | ETA: 5m 3s\n","Epoch  70/100 | LR: 0.001000 | Train Loss: 0.1347 | Train Acc: 95.62% | Val Loss: 0.1797 | Val Acc: 93.63% | Time: 7.6s | ETA: 3m 47s\n","Epoch  80/100 | LR: 0.001000 | Train Loss: 0.1341 | Train Acc: 95.62% | Val Loss: 0.1777 | Val Acc: 93.65% | Time: 7.5s | ETA: 2m 31s\n","Epoch  90/100 | LR: 0.000100 | Train Loss: 0.1308 | Train Acc: 95.85% | Val Loss: 0.1787 | Val Acc: 93.42% | Time: 7.5s | ETA: 1m 15s\n","Epoch 100/100 | LR: 0.000100 | Train Loss: 0.1292 | Train Acc: 95.92% | Val Loss: 0.1753 | Val Acc: 93.88% | Time: 7.8s | ETA: 0s\n","\n","‚úÖ Training complete!\n","üèÜ Best validation accuracy: 93.88%\n","‚è±Ô∏è  Total time: 12m 38s\n","======================================================================\n","\n","======================================================================\n","‚úÖ SAVED: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025/step_decay_seed123.json\n","‚è±Ô∏è  Experiment time: 12m 38s\n","üìä Progress: 4/10 experiments\n","‚è≥ ETA for remaining: 1h 15m 44s\n","======================================================================\n","\n","\n","######################################################################\n","üìç EXPERIMENT 5/10\n","üìã Schedule: exponential\n","üé≤ Seed: 42\n","######################################################################\n","\n","\n","======================================================================\n","Training: exponential | Seed: 42\n","======================================================================\n","\n","Epoch  10/100 | LR: 0.059874 | Train Loss: 0.2553 | Train Acc: 91.06% | Val Loss: 0.2573 | Val Acc: 90.50% | Time: 7.6s | ETA: 11m 21s\n","Epoch  20/100 | LR: 0.035849 | Train Loss: 0.2129 | Train Acc: 92.74% | Val Loss: 0.2310 | Val Acc: 91.38% | Time: 7.4s | ETA: 10m 4s\n","Epoch  30/100 | LR: 0.021464 | Train Loss: 0.1854 | Train Acc: 93.67% | Val Loss: 0.2583 | Val Acc: 90.85% | Time: 7.6s | ETA: 8m 50s\n","Epoch  40/100 | LR: 0.012851 | Train Loss: 0.1653 | Train Acc: 94.39% | Val Loss: 0.2224 | Val Acc: 91.77% | Time: 7.7s | ETA: 7m 34s\n","Epoch  50/100 | LR: 0.007694 | Train Loss: 0.1436 | Train Acc: 95.29% | Val Loss: 0.2056 | Val Acc: 92.47% | Time: 7.5s | ETA: 6m 19s\n","Epoch  60/100 | LR: 0.004607 | Train Loss: 0.1284 | Train Acc: 95.83% | Val Loss: 0.1807 | Val Acc: 93.75% | Time: 7.4s | ETA: 5m 3s\n","Epoch  70/100 | LR: 0.002758 | Train Loss: 0.1173 | Train Acc: 96.31% | Val Loss: 0.1784 | Val Acc: 93.57% | Time: 7.5s | ETA: 3m 46s\n","Epoch  80/100 | LR: 0.001652 | Train Loss: 0.1089 | Train Acc: 96.67% | Val Loss: 0.1769 | Val Acc: 93.55% | Time: 7.7s | ETA: 2m 31s\n","Epoch  90/100 | LR: 0.000989 | Train Loss: 0.1049 | Train Acc: 96.83% | Val Loss: 0.1787 | Val Acc: 93.52% | Time: 7.7s | ETA: 1m 15s\n","Epoch 100/100 | LR: 0.000592 | Train Loss: 0.1009 | Train Acc: 96.96% | Val Loss: 0.1754 | Val Acc: 93.65% | Time: 7.6s | ETA: 0s\n","\n","‚úÖ Training complete!\n","üèÜ Best validation accuracy: 93.98%\n","‚è±Ô∏è  Total time: 12m 39s\n","======================================================================\n","\n","======================================================================\n","‚úÖ SAVED: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025/exponential_seed42.json\n","‚è±Ô∏è  Experiment time: 12m 39s\n","üìä Progress: 5/10 experiments\n","‚è≥ ETA for remaining: 1h 3m 8s\n","======================================================================\n","\n","\n","######################################################################\n","üìç EXPERIMENT 6/10\n","üìã Schedule: exponential\n","üé≤ Seed: 123\n","######################################################################\n","\n","\n","======================================================================\n","Training: exponential | Seed: 123\n","======================================================================\n","\n","Epoch  10/100 | LR: 0.059874 | Train Loss: 0.2554 | Train Acc: 91.11% | Val Loss: 0.3239 | Val Acc: 88.63% | Time: 7.7s | ETA: 11m 21s\n","Epoch  20/100 | LR: 0.035849 | Train Loss: 0.2153 | Train Acc: 92.54% | Val Loss: 0.2198 | Val Acc: 92.12% | Time: 7.5s | ETA: 10m 4s\n","Epoch  30/100 | LR: 0.021464 | Train Loss: 0.1842 | Train Acc: 93.76% | Val Loss: 0.2077 | Val Acc: 92.47% | Time: 7.4s | ETA: 8m 50s\n","Epoch  40/100 | LR: 0.012851 | Train Loss: 0.1605 | Train Acc: 94.67% | Val Loss: 0.2061 | Val Acc: 92.30% | Time: 7.4s | ETA: 7m 34s\n","Epoch  50/100 | LR: 0.007694 | Train Loss: 0.1446 | Train Acc: 95.20% | Val Loss: 0.1950 | Val Acc: 92.90% | Time: 7.6s | ETA: 6m 18s\n","Epoch  60/100 | LR: 0.004607 | Train Loss: 0.1305 | Train Acc: 95.85% | Val Loss: 0.1789 | Val Acc: 93.30% | Time: 7.6s | ETA: 5m 2s\n","Epoch  70/100 | LR: 0.002758 | Train Loss: 0.1187 | Train Acc: 96.29% | Val Loss: 0.1840 | Val Acc: 93.30% | Time: 7.6s | ETA: 3m 46s\n","Epoch  80/100 | LR: 0.001652 | Train Loss: 0.1119 | Train Acc: 96.61% | Val Loss: 0.1842 | Val Acc: 93.30% | Time: 7.6s | ETA: 2m 31s\n","Epoch  90/100 | LR: 0.000989 | Train Loss: 0.1067 | Train Acc: 96.87% | Val Loss: 0.1801 | Val Acc: 93.48% | Time: 7.4s | ETA: 1m 15s\n","Epoch 100/100 | LR: 0.000592 | Train Loss: 0.1046 | Train Acc: 96.81% | Val Loss: 0.1765 | Val Acc: 93.28% | Time: 7.5s | ETA: 0s\n","\n","‚úÖ Training complete!\n","üèÜ Best validation accuracy: 93.55%\n","‚è±Ô∏è  Total time: 12m 35s\n","======================================================================\n","\n","======================================================================\n","‚úÖ SAVED: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025/exponential_seed123.json\n","‚è±Ô∏è  Experiment time: 12m 35s\n","üìä Progress: 6/10 experiments\n","‚è≥ ETA for remaining: 50m 29s\n","======================================================================\n","\n","\n","######################################################################\n","üìç EXPERIMENT 7/10\n","üìã Schedule: cosine\n","üé≤ Seed: 42\n","######################################################################\n","\n","\n","======================================================================\n","Training: cosine | Seed: 42\n","======================================================================\n","\n","Epoch  10/100 | LR: 0.097555 | Train Loss: 0.2694 | Train Acc: 90.54% | Val Loss: 0.3219 | Val Acc: 88.40% | Time: 7.7s | ETA: 11m 22s\n","Epoch  20/100 | LR: 0.090460 | Train Loss: 0.2481 | Train Acc: 91.42% | Val Loss: 0.3308 | Val Acc: 88.38% | Time: 7.5s | ETA: 10m 6s\n","Epoch  30/100 | LR: 0.079410 | Train Loss: 0.2298 | Train Acc: 92.02% | Val Loss: 0.2842 | Val Acc: 90.30% | Time: 7.4s | ETA: 8m 48s\n","Epoch  40/100 | LR: 0.065485 | Train Loss: 0.2146 | Train Acc: 92.56% | Val Loss: 0.2979 | Val Acc: 88.82% | Time: 7.6s | ETA: 7m 33s\n","Epoch  50/100 | LR: 0.050050 | Train Loss: 0.1999 | Train Acc: 93.15% | Val Loss: 0.3214 | Val Acc: 89.00% | Time: 7.4s | ETA: 6m 17s\n","Epoch  60/100 | LR: 0.034615 | Train Loss: 0.1805 | Train Acc: 93.74% | Val Loss: 0.2311 | Val Acc: 91.47% | Time: 7.6s | ETA: 5m 2s\n","Epoch  70/100 | LR: 0.020690 | Train Loss: 0.1598 | Train Acc: 94.62% | Val Loss: 0.1993 | Val Acc: 92.88% | Time: 7.7s | ETA: 3m 47s\n","Epoch  80/100 | LR: 0.009640 | Train Loss: 0.1317 | Train Acc: 95.70% | Val Loss: 0.1980 | Val Acc: 93.02% | Time: 7.7s | ETA: 2m 31s\n","Epoch  90/100 | LR: 0.002545 | Train Loss: 0.1098 | Train Acc: 96.58% | Val Loss: 0.1786 | Val Acc: 93.38% | Time: 7.7s | ETA: 1m 15s\n","Epoch 100/100 | LR: 0.000100 | Train Loss: 0.1011 | Train Acc: 97.00% | Val Loss: 0.1705 | Val Acc: 93.77% | Time: 7.7s | ETA: 0s\n","\n","‚úÖ Training complete!\n","üèÜ Best validation accuracy: 93.77%\n","‚è±Ô∏è  Total time: 12m 38s\n","======================================================================\n","\n","======================================================================\n","‚úÖ SAVED: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025/cosine_seed42.json\n","‚è±Ô∏è  Experiment time: 12m 38s\n","üìä Progress: 7/10 experiments\n","‚è≥ ETA for remaining: 37m 52s\n","======================================================================\n","\n","\n","######################################################################\n","üìç EXPERIMENT 8/10\n","üìã Schedule: cosine\n","üé≤ Seed: 123\n","######################################################################\n","\n","\n","======================================================================\n","Training: cosine | Seed: 123\n","======================================================================\n","\n","Epoch  10/100 | LR: 0.097555 | Train Loss: 0.2753 | Train Acc: 90.39% | Val Loss: 0.2895 | Val Acc: 89.10% | Time: 7.4s | ETA: 11m 21s\n","Epoch  20/100 | LR: 0.090460 | Train Loss: 0.2446 | Train Acc: 91.42% | Val Loss: 0.3981 | Val Acc: 85.38% | Time: 7.6s | ETA: 10m 6s\n","Epoch  30/100 | LR: 0.079410 | Train Loss: 0.2286 | Train Acc: 92.09% | Val Loss: 0.3008 | Val Acc: 89.13% | Time: 7.7s | ETA: 8m 50s\n","Epoch  40/100 | LR: 0.065485 | Train Loss: 0.2142 | Train Acc: 92.56% | Val Loss: 0.2736 | Val Acc: 89.32% | Time: 7.6s | ETA: 7m 34s\n","Epoch  50/100 | LR: 0.050050 | Train Loss: 0.2036 | Train Acc: 92.89% | Val Loss: 0.2227 | Val Acc: 92.15% | Time: 7.6s | ETA: 6m 18s\n","Epoch  60/100 | LR: 0.034615 | Train Loss: 0.1826 | Train Acc: 93.86% | Val Loss: 0.2175 | Val Acc: 91.93% | Time: 7.8s | ETA: 5m 3s\n","Epoch  70/100 | LR: 0.020690 | Train Loss: 0.1611 | Train Acc: 94.57% | Val Loss: 0.2178 | Val Acc: 92.50% | Time: 7.4s | ETA: 3m 47s\n","Epoch  80/100 | LR: 0.009640 | Train Loss: 0.1371 | Train Acc: 95.55% | Val Loss: 0.2081 | Val Acc: 92.40% | Time: 7.6s | ETA: 2m 31s\n","Epoch  90/100 | LR: 0.002545 | Train Loss: 0.1135 | Train Acc: 96.44% | Val Loss: 0.1772 | Val Acc: 93.48% | Time: 7.4s | ETA: 1m 15s\n","Epoch 100/100 | LR: 0.000100 | Train Loss: 0.1041 | Train Acc: 96.94% | Val Loss: 0.1724 | Val Acc: 94.02% | Time: 7.5s | ETA: 0s\n","\n","‚úÖ Training complete!\n","üèÜ Best validation accuracy: 94.02%\n","‚è±Ô∏è  Total time: 12m 36s\n","======================================================================\n","\n","======================================================================\n","‚úÖ SAVED: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025/cosine_seed123.json\n","‚è±Ô∏è  Experiment time: 12m 36s\n","üìä Progress: 8/10 experiments\n","‚è≥ ETA for remaining: 25m 15s\n","======================================================================\n","\n","\n","######################################################################\n","üìç EXPERIMENT 9/10\n","üìã Schedule: warm_restarts\n","üé≤ Seed: 42\n","######################################################################\n","\n","\n","======================================================================\n","Training: warm_restarts | Seed: 42\n","======================================================================\n","\n","Epoch  10/100 | LR: 0.065485 | Train Loss: 0.2584 | Train Acc: 91.07% | Val Loss: 0.2756 | Val Acc: 90.40% | Time: 7.5s | ETA: 11m 23s\n","Epoch  20/100 | LR: 0.009640 | Train Loss: 0.1857 | Train Acc: 93.66% | Val Loss: 0.2016 | Val Acc: 92.37% | Time: 7.7s | ETA: 10m 7s\n","Epoch  30/100 | LR: 0.090460 | Train Loss: 0.2489 | Train Acc: 91.42% | Val Loss: 0.2587 | Val Acc: 90.32% | Time: 7.8s | ETA: 8m 51s\n","Epoch  40/100 | LR: 0.034615 | Train Loss: 0.2000 | Train Acc: 93.08% | Val Loss: 0.2215 | Val Acc: 91.72% | Time: 7.7s | ETA: 7m 35s\n","Epoch  50/100 | LR: 0.100000 | Train Loss: 0.1446 | Train Acc: 95.26% | Val Loss: 0.1742 | Val Acc: 93.32% | Time: 7.7s | ETA: 6m 19s\n","Epoch  60/100 | LR: 0.065485 | Train Loss: 0.2179 | Train Acc: 92.41% | Val Loss: 0.3428 | Val Acc: 88.10% | Time: 7.7s | ETA: 5m 3s\n","Epoch  70/100 | LR: 0.009640 | Train Loss: 0.1577 | Train Acc: 94.74% | Val Loss: 0.1914 | Val Acc: 92.77% | Time: 7.8s | ETA: 3m 47s\n","Epoch  80/100 | LR: 0.090460 | Train Loss: 0.2377 | Train Acc: 91.71% | Val Loss: 0.2716 | Val Acc: 90.05% | Time: 7.9s | ETA: 2m 31s\n","Epoch  90/100 | LR: 0.034615 | Train Loss: 0.1867 | Train Acc: 93.59% | Val Loss: 0.2581 | Val Acc: 91.07% | Time: 7.5s | ETA: 1m 15s\n","Epoch 100/100 | LR: 0.100000 | Train Loss: 0.1333 | Train Acc: 95.70% | Val Loss: 0.1723 | Val Acc: 93.58% | Time: 7.4s | ETA: 0s\n","\n","‚úÖ Training complete!\n","üèÜ Best validation accuracy: 93.73%\n","‚è±Ô∏è  Total time: 12m 38s\n","======================================================================\n","\n","======================================================================\n","‚úÖ SAVED: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025/warm_restarts_seed42.json\n","‚è±Ô∏è  Experiment time: 12m 38s\n","üìä Progress: 9/10 experiments\n","‚è≥ ETA for remaining: 12m 37s\n","======================================================================\n","\n","\n","######################################################################\n","üìç EXPERIMENT 10/10\n","üìã Schedule: warm_restarts\n","üé≤ Seed: 123\n","######################################################################\n","\n","\n","======================================================================\n","Training: warm_restarts | Seed: 123\n","======================================================================\n","\n","Epoch  10/100 | LR: 0.065485 | Train Loss: 0.2578 | Train Acc: 91.03% | Val Loss: 0.3131 | Val Acc: 88.25% | Time: 7.7s | ETA: 11m 26s\n","Epoch  20/100 | LR: 0.009640 | Train Loss: 0.1905 | Train Acc: 93.51% | Val Loss: 0.2196 | Val Acc: 92.28% | Time: 7.5s | ETA: 10m 8s\n","Epoch  30/100 | LR: 0.090460 | Train Loss: 0.2466 | Train Acc: 91.33% | Val Loss: 0.2762 | Val Acc: 89.93% | Time: 7.4s | ETA: 8m 53s\n","Epoch  40/100 | LR: 0.034615 | Train Loss: 0.1987 | Train Acc: 93.26% | Val Loss: 0.2690 | Val Acc: 89.95% | Time: 7.4s | ETA: 7m 36s\n","Epoch  50/100 | LR: 0.100000 | Train Loss: 0.1466 | Train Acc: 95.17% | Val Loss: 0.1799 | Val Acc: 93.45% | Time: 7.6s | ETA: 6m 19s\n","Epoch  60/100 | LR: 0.065485 | Train Loss: 0.2196 | Train Acc: 92.40% | Val Loss: 0.2496 | Val Acc: 91.12% | Time: 7.5s | ETA: 5m 3s\n","Epoch  70/100 | LR: 0.009640 | Train Loss: 0.1560 | Train Acc: 94.77% | Val Loss: 0.1970 | Val Acc: 92.68% | Time: 7.5s | ETA: 3m 47s\n","Epoch  80/100 | LR: 0.090460 | Train Loss: 0.2339 | Train Acc: 91.89% | Val Loss: 0.2548 | Val Acc: 90.37% | Time: 7.6s | ETA: 2m 31s\n","Epoch  90/100 | LR: 0.034615 | Train Loss: 0.1953 | Train Acc: 93.24% | Val Loss: 0.2248 | Val Acc: 91.83% | Time: 7.4s | ETA: 1m 15s\n","Epoch 100/100 | LR: 0.100000 | Train Loss: 0.1395 | Train Acc: 95.51% | Val Loss: 0.1772 | Val Acc: 93.65% | Time: 7.5s | ETA: 0s\n","\n","‚úÖ Training complete!\n","üèÜ Best validation accuracy: 93.82%\n","‚è±Ô∏è  Total time: 12m 38s\n","======================================================================\n","\n","======================================================================\n","‚úÖ SAVED: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025/warm_restarts_seed123.json\n","‚è±Ô∏è  Experiment time: 12m 38s\n","üìä Progress: 10/10 experiments\n","‚è≥ ETA for remaining: 0s\n","======================================================================\n","\n","\n","üéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ\n","ALL EXPERIMENTS COMPLETE!\n","üéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâüéâ\n","\n","‚úÖ Total experiments completed: 10\n","‚è±Ô∏è  Total time: 2h 6m 16s\n","‚ö° Average time per experiment: 12m 37s\n","üìÅ Results saved to: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025\n","\n","======================================================================\n","\n","üìä Result files:\n","   ‚úì constant_seed123.json\n","   ‚úì constant_seed42.json\n","   ‚úì cosine_seed123.json\n","   ‚úì cosine_seed42.json\n","   ‚úì exponential_seed123.json\n","   ‚úì exponential_seed42.json\n","   ‚úì step_decay_seed123.json\n","   ‚úì step_decay_seed42.json\n","   ‚úì warm_restarts_seed123.json\n","   ‚úì warm_restarts_seed42.json\n","\n","======================================================================\n","‚úÖ Ready for visualization and analysis!\n","======================================================================\n"]}],"source":["# ============================================================================\n","# RUN ALL 10 EXPERIMENTS\n","# ============================================================================\n","\n","\"\"\"\n","This cell runs all 10 experiments:\n","- 5 schedules √ó 2 random seeds = 10 experiments\n","- 100 epochs per experiment\n","- Total: 1,000 epochs of training\n","\n","Expected runtime on L4 GPU: ~1.5 hours\n","- ~9 minutes per experiment\n","- Saves after each experiment (safe from disconnects)\n","\n","Progress will be displayed for each experiment.\n","\"\"\"\n","\n","# Experimental configuration\n","SCHEDULES = ['constant', 'step_decay', 'exponential', 'cosine', 'warm_restarts']\n","SEEDS = [42, 123]\n","EPOCHS = 100\n","INITIAL_LR = 0.1\n","BATCH_SIZE = 256\n","\n","# Training hyperparameters\n","# Using SGD as it responds well to LR scheduling (unlike adaptive optimizers)\n","MOMENTUM = 0.9\n","WEIGHT_DECAY = 5e-4\n","\n","print(\"=\"*70)\n","print(\"EXPERIMENTAL SETUP\")\n","print(\"=\"*70)\n","print(f\"Schedules: {SCHEDULES}\")\n","print(f\"Seeds: {SEEDS}\")\n","print(f\"Epochs per experiment: {EPOCHS}\")\n","print(f\"Total experiments: {len(SCHEDULES) * len(SEEDS)}\")\n","print(f\"Total epochs: {len(SCHEDULES) * len(SEEDS) * EPOCHS}\")\n","print(f\"\\nOptimizer: SGD\")\n","print(f\"  - Learning rate: {INITIAL_LR}\")\n","print(f\"  - Momentum: {MOMENTUM}\")\n","print(f\"  - Weight decay: {WEIGHT_DECAY}\")\n","print(f\"\\nBatch size: {BATCH_SIZE}\")\n","print(f\"Results directory: {RESULTS_DIR}\")\n","print(\"=\"*70)\n","\n","# Check what's already completed\n","existing_files = os.listdir(RESULTS_DIR) if os.path.exists(RESULTS_DIR) else []\n","print(f\"\\nüìÅ Existing results: {len(existing_files)} files\")\n","\n","# Confirm before starting\n","print(f\"\\n‚ö†Ô∏è  This will take approximately 1.5 hours on L4 GPU.\")\n","print(f\"üí° Results are saved after each experiment.\")\n","print(f\"üîÑ You can safely stop and resume if needed.\")\n","print(f\"\\n{'='*70}\\n\")\n","\n","# Run all experiments\n","all_results = {}\n","experiment_count = 0\n","total_experiments = len(SCHEDULES) * len(SEEDS)\n","overall_start_time = time.time()\n","\n","for schedule_name in SCHEDULES:\n","    for seed in SEEDS:\n","        experiment_count += 1\n","\n","        print(f\"\\n{'#'*70}\")\n","        print(f\"üìç EXPERIMENT {experiment_count}/{total_experiments}\")\n","        print(f\"üìã Schedule: {schedule_name}\")\n","        print(f\"üé≤ Seed: {seed}\")\n","        print(f\"{'#'*70}\\n\")\n","\n","        # Set seed for reproducibility\n","        set_seed(seed)\n","\n","        # Create model\n","        model = get_small_cnn().to(device)\n","\n","        # Create optimizer (SGD with momentum)\n","        optimizer = optim.SGD(\n","            model.parameters(),\n","            lr=INITIAL_LR,\n","            momentum=MOMENTUM,\n","            weight_decay=WEIGHT_DECAY\n","        )\n","\n","        # Create scheduler\n","        scheduler = get_scheduler(optimizer, schedule_name, EPOCHS)\n","\n","        # Loss function\n","        criterion = nn.CrossEntropyLoss()\n","\n","        # Train\n","        exp_start = time.time()\n","        history = train_with_schedule(\n","            model, train_loader, val_loader, optimizer,\n","            scheduler, criterion, device, EPOCHS,\n","            schedule_name, seed\n","        )\n","        exp_time = time.time() - exp_start\n","\n","        # Save results immediately\n","        filename = f\"{schedule_name}_seed{seed}.json\"\n","        filepath = os.path.join(RESULTS_DIR, filename)\n","\n","        with open(filepath, 'w') as f:\n","            json.dump(history, f, indent=2)\n","\n","        print(f\"{'='*70}\")\n","        print(f\"‚úÖ SAVED: {filepath}\")\n","        print(f\"‚è±Ô∏è  Experiment time: {format_time(exp_time)}\")\n","\n","        # Calculate ETA\n","        elapsed_total = time.time() - overall_start_time\n","        avg_time_per_exp = elapsed_total / experiment_count\n","        remaining_exps = total_experiments - experiment_count\n","        eta_total = avg_time_per_exp * remaining_exps\n","\n","        print(f\"üìä Progress: {experiment_count}/{total_experiments} experiments\")\n","        print(f\"‚è≥ ETA for remaining: {format_time(eta_total)}\")\n","        print(f\"{'='*70}\\n\")\n","\n","        # Store in memory\n","        all_results[f\"{schedule_name}_seed{seed}\"] = history\n","\n","        # Clean up GPU memory\n","        del model, optimizer, scheduler\n","        torch.cuda.empty_cache()\n","\n","# Final summary\n","total_time = time.time() - overall_start_time\n","\n","print(\"\\n\" + \"üéâ\"*35)\n","print(\"ALL EXPERIMENTS COMPLETE!\")\n","print(\"üéâ\"*35 + \"\\n\")\n","print(f\"‚úÖ Total experiments completed: {total_experiments}\")\n","print(f\"‚è±Ô∏è  Total time: {format_time(total_time)}\")\n","print(f\"‚ö° Average time per experiment: {format_time(total_time/total_experiments)}\")\n","print(f\"üìÅ Results saved to: {RESULTS_DIR}\")\n","print(f\"\\n{'='*70}\\n\")\n","\n","# List all result files\n","print(\"üìä Result files:\")\n","result_files = sorted([f for f in os.listdir(RESULTS_DIR) if f.endswith('.json')])\n","for f in result_files:\n","    print(f\"   ‚úì {f}\")\n","\n","print(f\"\\n{'='*70}\")\n","print(\"‚úÖ Ready for visualization and analysis!\")\n","print(\"=\"*70)"]},{"cell_type":"code","source":["# ============================================================================\n","# GENERATE COLORBLIND-FRIENDLY VISUALIZATIONS\n","# ============================================================================\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np\n","import json\n","import os\n","\n","# Set correct path\n","RESULTS_DIR = '/content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025'\n","\n","# ENHANCED Colorblind-friendly palette (Okabe-Ito palette)\n","colors = ['#0173B2', '#DE8F05', '#029E73', '#CC78BC', '#CA9161']\n","schedules = ['constant', 'step_decay', 'exponential', 'cosine', 'warm_restarts']\n","schedule_names = ['Constant', 'Step Decay', 'Exponential', 'Cosine Annealing', 'Warm Restarts']\n","\n","# DIFFERENT LINE STYLES (key for colorblind accessibility!)\n","linestyles = ['-', '--', '-.', ':', (0, (3, 1, 1, 1))]  # solid, dashed, dash-dot, dotted, custom\n","\n","# DIFFERENT MARKERS (additional visual cue!)\n","markers = ['o', 's', '^', 'D', 'v']  # circle, square, triangle-up, diamond, triangle-down\n","marker_sizes = [6, 6, 7, 6, 7]\n","\n","print(\"üé® Generating COLORBLIND-FRIENDLY visualizations...\")\n","print(\"=\"*70)\n","\n","# Load all data\n","all_data = {}\n","for schedule in schedules:\n","    all_data[schedule] = []\n","    for seed in [42, 123]:\n","        filepath = os.path.join(RESULTS_DIR, f'{schedule}_seed{seed}.json')\n","        with open(filepath) as f:\n","            all_data[schedule].append(json.load(f))\n","\n","print(\"‚úÖ Loaded all experimental results\")\n","\n","# ============================================================================\n","# FIGURE 1: LEARNING RATE SCHEDULES (WITH MARKERS)\n","# ============================================================================\n","\n","print(\"Creating Figure 1: Learning Rate Schedules (colorblind-friendly)...\")\n","\n","fig, axes = plt.subplots(1, 5, figsize=(22, 4))\n","\n","for idx, schedule in enumerate(schedules):\n","    data = all_data[schedule][0]  # Use seed 42\n","    epochs = data['epoch']\n","    lrs = data['lr']\n","\n","    # Plot with thicker lines and markers every 10 epochs\n","    axes[idx].plot(epochs, lrs,\n","                  linewidth=3.5,  # Thicker for visibility\n","                  color=colors[idx],\n","                  linestyle=linestyles[idx],\n","                  marker=markers[idx],\n","                  markevery=10,  # Show marker every 10 epochs\n","                  markersize=marker_sizes[idx],\n","                  markeredgecolor='white',\n","                  markeredgewidth=1)\n","\n","    axes[idx].set_title(schedule_names[idx], fontsize=13, fontweight='bold', color=colors[idx])\n","    axes[idx].set_xlabel('Epoch', fontsize=11, fontweight='bold')\n","    if idx == 0:\n","        axes[idx].set_ylabel('Learning Rate', fontsize=11, fontweight='bold')\n","    axes[idx].grid(True, alpha=0.3, linewidth=0.8)\n","    axes[idx].set_yscale('log')\n","    axes[idx].tick_params(labelsize=10)\n","\n","plt.suptitle('Learning Rate Schedules Comparison (Colorblind-Friendly)',\n","             fontsize=16, fontweight='bold', y=1.02)\n","plt.tight_layout()\n","plt.savefig(os.path.join(RESULTS_DIR, 'lr_schedules.png'), dpi=300, bbox_inches='tight')\n","print(\"‚úÖ Saved: lr_schedules.png\")\n","plt.close()\n","\n","# ============================================================================\n","# FIGURE 2: VALIDATION ACCURACY (WITH MARKERS & LINE STYLES)\n","# ============================================================================\n","\n","print(\"Creating Figure 2: Validation Accuracy (colorblind-friendly)...\")\n","\n","fig, ax = plt.subplots(figsize=(14, 8))\n","\n","for idx, schedule in enumerate(schedules):\n","    val_accs = [data['val_acc'] for data in all_data[schedule]]\n","    epochs = all_data[schedule][0]['epoch']\n","\n","    mean_va = np.mean(val_accs, axis=0)\n","    std_va = np.std(val_accs, axis=0)\n","\n","    # Main line with marker\n","    ax.plot(epochs, mean_va,\n","           label=schedule_names[idx],\n","           color=colors[idx],\n","           linewidth=4,  # Thick line\n","           linestyle=linestyles[idx],  # Different styles\n","           marker=markers[idx],  # Different markers\n","           markevery=10,  # Marker every 10 epochs\n","           markersize=marker_sizes[idx]+2,\n","           markeredgecolor='white',\n","           markeredgewidth=1.5,\n","           alpha=0.9)\n","\n","    # Shaded error region\n","    ax.fill_between(epochs, mean_va-std_va, mean_va+std_va,\n","                    alpha=0.15, color=colors[idx])\n","\n","    # Final accuracy annotation with background\n","    final_acc = mean_va[-1]\n","    ax.text(102, final_acc, f'{final_acc:.2f}%',\n","           fontsize=11, fontweight='bold', color=colors[idx],\n","           va='center',\n","           bbox=dict(boxstyle='round,pad=0.3', facecolor='white',\n","                    edgecolor=colors[idx], linewidth=2, alpha=0.9))\n","\n","ax.set_xlabel('Epoch', fontsize=14, fontweight='bold')\n","ax.set_ylabel('Validation Accuracy (%)', fontsize=14, fontweight='bold')\n","ax.set_title('Validation Accuracy: Learning Rate Schedule Comparison\\n(Colorblind-Friendly: Different line styles & markers)',\n","            fontsize=15, fontweight='bold', pad=15)\n","ax.legend(fontsize=12, loc='lower right', framealpha=0.98,\n","         edgecolor='black', fancybox=True, shadow=True)\n","ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n","ax.set_ylim([86, 95])\n","ax.set_xlim([0, 105])\n","ax.tick_params(labelsize=12)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(RESULTS_DIR, 'validation_accuracy_comparison.png'), dpi=300, bbox_inches='tight')\n","print(\"‚úÖ Saved: validation_accuracy_comparison.png\")\n","plt.close()\n","\n","# ============================================================================\n","# FIGURE 3: TRAINING ACCURACY (WITH MARKERS & LINE STYLES)\n","# ============================================================================\n","\n","print(\"Creating Figure 3: Training Accuracy (colorblind-friendly)...\")\n","\n","fig, ax = plt.subplots(figsize=(14, 8))\n","\n","for idx, schedule in enumerate(schedules):\n","    train_accs = [data['train_acc'] for data in all_data[schedule]]\n","    epochs = all_data[schedule][0]['epoch']\n","\n","    mean_ta = np.mean(train_accs, axis=0)\n","    std_ta = np.std(train_accs, axis=0)\n","\n","    ax.plot(epochs, mean_ta,\n","           label=schedule_names[idx],\n","           color=colors[idx],\n","           linewidth=4,\n","           linestyle=linestyles[idx],\n","           marker=markers[idx],\n","           markevery=10,\n","           markersize=marker_sizes[idx]+2,\n","           markeredgecolor='white',\n","           markeredgewidth=1.5,\n","           alpha=0.9)\n","\n","    ax.fill_between(epochs, mean_ta-std_ta, mean_ta+std_ta,\n","                    alpha=0.15, color=colors[idx])\n","\n","ax.set_xlabel('Epoch', fontsize=14, fontweight='bold')\n","ax.set_ylabel('Training Accuracy (%)', fontsize=14, fontweight='bold')\n","ax.set_title('Training Accuracy Comparison\\n(Colorblind-Friendly: Different line styles & markers)',\n","            fontsize=15, fontweight='bold', pad=15)\n","ax.legend(fontsize=12, loc='lower right', framealpha=0.98,\n","         edgecolor='black', fancybox=True, shadow=True)\n","ax.grid(True, alpha=0.3, linestyle='--', linewidth=0.8)\n","ax.set_ylim([85, 98])\n","ax.tick_params(labelsize=12)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(RESULTS_DIR, 'training_accuracy_comparison.png'), dpi=300, bbox_inches='tight')\n","print(\"‚úÖ Saved: training_accuracy_comparison.png\")\n","plt.close()\n","\n","# ============================================================================\n","# FIGURE 4: BAR CHART WITH PATTERNS (COLORBLIND-FRIENDLY)\n","# ============================================================================\n","\n","print(\"Creating Figure 4: Bar Chart (with patterns for colorblind accessibility)...\")\n","\n","final_accs = []\n","errors = []\n","\n","for schedule in schedules:\n","    accs = []\n","    for data in all_data[schedule]:\n","        accs.append(np.mean(data['val_acc'][-5:]))\n","    final_accs.append(np.mean(accs))\n","    errors.append(np.std(accs))\n","\n","fig, ax = plt.subplots(figsize=(13, 8))\n","x_pos = np.arange(len(schedules))\n","\n","# Different hatch patterns for each bar (colorblind accessibility!)\n","hatches = ['', '//', '\\\\\\\\', 'xx', '++']\n","\n","bars = []\n","for i, (acc, err, hatch) in enumerate(zip(final_accs, errors, hatches)):\n","    bar = ax.bar(x_pos[i], acc, yerr=err, capsize=10,\n","                color=colors[i], alpha=0.85,\n","                edgecolor='black', linewidth=2.5,\n","                hatch=hatch)  # Different pattern for each\n","    bars.append(bar)\n","\n","ax.set_ylabel('Final Validation Accuracy (%)', fontsize=14, fontweight='bold')\n","ax.set_title('Learning Rate Schedule Performance on Fashion-MNIST\\n(Colorblind-Friendly: Different patterns)',\n","            fontsize=15, fontweight='bold', pad=15)\n","ax.set_xticks(x_pos)\n","ax.set_xticklabels(schedule_names, rotation=20, ha='right', fontsize=12, fontweight='bold')\n","ax.set_ylim([87, 95])\n","ax.grid(True, alpha=0.3, axis='y', linewidth=0.8)\n","ax.tick_params(labelsize=12)\n","\n","# Value labels on bars with background\n","for i, (bar, acc) in enumerate(zip(bars, final_accs)):\n","    height = bar[0].get_height()\n","    ax.text(x_pos[i], height + 0.35,\n","           f'{acc:.2f}%',\n","           ha='center', va='bottom', fontweight='bold', fontsize=13,\n","           bbox=dict(boxstyle='round,pad=0.4', facecolor='white',\n","                    edgecolor=colors[i], linewidth=2))\n","\n","# Highlight best with THICK gold border\n","best_idx = np.argmax(final_accs)\n","bars[best_idx][0].set_edgecolor('gold')\n","bars[best_idx][0].set_linewidth(5)\n","\n","# Add legend explaining patterns\n","from matplotlib.patches import Patch\n","legend_elements = [Patch(facecolor=colors[i], edgecolor='black',\n","                        linewidth=2, hatch=hatches[i], label=schedule_names[i])\n","                  for i in range(len(schedules))]\n","ax.legend(handles=legend_elements, loc='upper left', fontsize=11,\n","         framealpha=0.98, edgecolor='black')\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(RESULTS_DIR, 'final_performance_bar.png'), dpi=300, bbox_inches='tight')\n","print(\"‚úÖ Saved: final_performance_bar.png\")\n","plt.close()\n","\n","# ============================================================================\n","# SUMMARY & VERIFICATION\n","# ============================================================================\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚úÖ ALL COLORBLIND-FRIENDLY VISUALIZATIONS GENERATED!\")\n","print(\"=\"*70)\n","print(\"\\nüé® Accessibility Features Added:\")\n","print(\"  ‚úì Different line styles (solid, dashed, dotted, etc.)\")\n","print(\"  ‚úì Different markers (circle, square, triangle, diamond)\")\n","print(\"  ‚úì Different hatch patterns in bar chart\")\n","print(\"  ‚úì Thicker lines (4px) for better visibility\")\n","print(\"  ‚úì White marker edges for contrast\")\n","print(\"  ‚úì Text boxes with colored borders\")\n","print(\"  ‚úì Gold border on winner bar\")\n","print(\"  ‚úì Okabe-Ito colorblind-safe palette\")\n","print(\"\\nüìä Generated files:\")\n","print(\"  1. lr_schedules.png\")\n","print(\"  2. validation_accuracy_comparison.png\")\n","print(\"  3. training_accuracy_comparison.png\")\n","print(\"  4. final_performance_bar.png\")\n","print(f\"\\nSaved to: {RESULTS_DIR}\")\n","print(\"=\"*70)\n","\n","# Verify files were created\n","viz_files = [f for f in os.listdir(RESULTS_DIR) if f.endswith('.png')]\n","print(f\"\\n‚úÖ Total PNG files: {len(viz_files)}\")\n","for f in sorted(viz_files):\n","    size = os.path.getsize(os.path.join(RESULTS_DIR, f)) / 1024\n","    print(f\"  ‚úì {f} ({size:.1f} KB)\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚ôø COLORBLIND ACCESSIBILITY VERIFIED!\")\n","print(\"=\"*70)\n","print(\"These plots can be distinguished by:\")\n","print(\"  ‚Ä¢ Color (for color vision)\")\n","print(\"  ‚Ä¢ Line style (for colorblind users)\")\n","print(\"  ‚Ä¢ Markers (additional visual cue)\")\n","print(\"  ‚Ä¢ Patterns (in bar chart)\")\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L3ozM3lwOkN2","executionInfo":{"status":"ok","timestamp":1765386388205,"user_tz":0,"elapsed":11769,"user":{"displayName":"Ibitunde Mayowa","userId":"12368864409965566243"}},"outputId":"be5abd20-c81d-4e38-f3cd-a937c5a14212"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["üé® Generating COLORBLIND-FRIENDLY visualizations...\n","======================================================================\n","‚úÖ Loaded all experimental results\n","Creating Figure 1: Learning Rate Schedules (colorblind-friendly)...\n","‚úÖ Saved: lr_schedules.png\n","Creating Figure 2: Validation Accuracy (colorblind-friendly)...\n","‚úÖ Saved: validation_accuracy_comparison.png\n","Creating Figure 3: Training Accuracy (colorblind-friendly)...\n","‚úÖ Saved: training_accuracy_comparison.png\n","Creating Figure 4: Bar Chart (with patterns for colorblind accessibility)...\n","‚úÖ Saved: final_performance_bar.png\n","\n","======================================================================\n","‚úÖ ALL COLORBLIND-FRIENDLY VISUALIZATIONS GENERATED!\n","======================================================================\n","\n","üé® Accessibility Features Added:\n","  ‚úì Different line styles (solid, dashed, dotted, etc.)\n","  ‚úì Different markers (circle, square, triangle, diamond)\n","  ‚úì Different hatch patterns in bar chart\n","  ‚úì Thicker lines (4px) for better visibility\n","  ‚úì White marker edges for contrast\n","  ‚úì Text boxes with colored borders\n","  ‚úì Gold border on winner bar\n","  ‚úì Okabe-Ito colorblind-safe palette\n","\n","üìä Generated files:\n","  1. lr_schedules.png\n","  2. validation_accuracy_comparison.png\n","  3. training_accuracy_comparison.png\n","  4. final_performance_bar.png\n","\n","Saved to: /content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025\n","======================================================================\n","\n","‚úÖ Total PNG files: 4\n","  ‚úì final_performance_bar.png (449.0 KB)\n","  ‚úì lr_schedules.png (280.3 KB)\n","  ‚úì training_accuracy_comparison.png (808.2 KB)\n","  ‚úì validation_accuracy_comparison.png (2192.6 KB)\n","\n","======================================================================\n","‚ôø COLORBLIND ACCESSIBILITY VERIFIED!\n","======================================================================\n","These plots can be distinguished by:\n","  ‚Ä¢ Color (for color vision)\n","  ‚Ä¢ Line style (for colorblind users)\n","  ‚Ä¢ Markers (additional visual cue)\n","  ‚Ä¢ Patterns (in bar chart)\n","======================================================================\n"]}]},{"cell_type":"code","source":["import os\n","import json\n","\n","# CORRECT PATH\n","RESULTS_DIR = '/content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025'\n","\n","print(\"=\"*70)\n","print(\"CHECKING RESULTS\")\n","print(\"=\"*70)\n","\n","# List all files\n","files = sorted(os.listdir(RESULTS_DIR))\n","\n","print(f\"\\nTotal files: {len(files)}\")\n","print(\"\\nJSON result files:\")\n","json_files = [f for f in files if f.endswith('.json')]\n","for f in json_files:\n","    size = os.path.getsize(os.path.join(RESULTS_DIR, f)) / 1024\n","    print(f\"  ‚úì {f} ({size:.1f} KB)\")\n","\n","print(f\"\\nTotal JSON files: {len(json_files)}/10\")\n","\n","print(\"\\nVisualization files:\")\n","image_files = [f for f in files if f.endswith('.png')]\n","for f in image_files:\n","    size = os.path.getsize(os.path.join(RESULTS_DIR, f)) / 1024\n","    print(f\"  ‚úì {f} ({size:.1f} KB)\")\n","\n","print(f\"\\nTotal images: {len(image_files)}\")\n","\n","print(\"\\nOther files:\")\n","other_files = [f for f in files if not f.endswith('.json') and not f.endswith('.png')]\n","for f in other_files:\n","    print(f\"  ‚úì {f}\")\n","\n","print(\"=\"*70)\n","\n","if len(json_files) == 10:\n","    print(\"‚úÖ ALL EXPERIMENTS COMPLETED!\")\n","else:\n","    print(f\"‚ö†Ô∏è  WARNING: Only {len(json_files)}/10 experiments found!\")\n","\n","if len(image_files) >= 4:\n","    print(\"‚úÖ ALL VISUALIZATIONS GENERATED!\")\n","else:\n","    print(f\"‚ö†Ô∏è  WARNING: Only {len(image_files)} images found!\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"READY TO DOWNLOAD!\")\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q9RJXSvEH6VC","executionInfo":{"status":"ok","timestamp":1765386399222,"user_tz":0,"elapsed":43,"user":{"displayName":"Ibitunde Mayowa","userId":"12368864409965566243"}},"outputId":"937daec1-e65a-4fe2-8510-0050f8dd8e3a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","CHECKING RESULTS\n","======================================================================\n","\n","Total files: 14\n","\n","JSON result files:\n","  ‚úì constant_seed123.json (12.9 KB)\n","  ‚úì constant_seed42.json (12.8 KB)\n","  ‚úì cosine_seed123.json (14.4 KB)\n","  ‚úì cosine_seed42.json (14.4 KB)\n","  ‚úì exponential_seed123.json (14.5 KB)\n","  ‚úì exponential_seed42.json (14.5 KB)\n","  ‚úì step_decay_seed123.json (14.0 KB)\n","  ‚úì step_decay_seed42.json (14.1 KB)\n","  ‚úì warm_restarts_seed123.json (14.4 KB)\n","  ‚úì warm_restarts_seed42.json (14.3 KB)\n","\n","Total JSON files: 10/10\n","\n","Visualization files:\n","  ‚úì final_performance_bar.png (449.0 KB)\n","  ‚úì lr_schedules.png (280.3 KB)\n","  ‚úì training_accuracy_comparison.png (808.2 KB)\n","  ‚úì validation_accuracy_comparison.png (2192.6 KB)\n","\n","Total images: 4\n","\n","Other files:\n","======================================================================\n","‚úÖ ALL EXPERIMENTS COMPLETED!\n","‚úÖ ALL VISUALIZATIONS GENERATED!\n","\n","======================================================================\n","READY TO DOWNLOAD!\n","======================================================================\n"]}]},{"cell_type":"code","source":["# ============================================================================\n","# CREATE ZIP FILE FOR DOWNLOAD\n","# ============================================================================\n","\n","import shutil\n","import os\n","\n","RESULTS_DIR = '/content/drive/MyDrive/LR_Schedules_FashionMNIST_Final_Dec2025'\n","zip_path = '/content/LR_Schedules_Complete'\n","\n","print(\"=\"*70)\n","print(\"CREATING ZIP FILE FOR DOWNLOAD\")\n","print(\"=\"*70)\n","\n","# Create zip\n","print(\"\\nüì¶ Zipping all results...\")\n","shutil.make_archive(zip_path, 'zip', RESULTS_DIR)\n","\n","zip_size = os.path.getsize(zip_path + '.zip') / (1024*1024)\n","print(f\"‚úÖ Created: LR_Schedules_Complete.zip\")\n","print(f\"üì¶ Size: {zip_size:.1f} MB\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"TO DOWNLOAD:\")\n","print(\"=\"*70)\n","print(\"1. Click the folder icon üìÅ in the LEFT sidebar of Colab\")\n","print(\"2. You'll see the file browser\")\n","print(\"3. Navigate to: /content/\")\n","print(\"4. Find: LR_Schedules_Complete.zip\")\n","print(\"5. Right-click on it ‚Üí Download\")\n","print(\"=\"*70)\n","\n","print(\"\\nüìã What's inside the zip:\")\n","print(\"  ‚Ä¢ 10 JSON files (experiment results)\")\n","print(\"  ‚Ä¢ 4 PNG files (colorblind-friendly visualizations)\")\n","print(\"=\"*70)\n","\n","print(\"\\nüí° TIP: The zip file is in /content (temporary)\")\n","print(\"   It will disappear when you close Colab\")\n","print(\"   Download it NOW!\")\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4fDh5pYMmC-","executionInfo":{"status":"ok","timestamp":1765386869622,"user_tz":0,"elapsed":217,"user":{"displayName":"Ibitunde Mayowa","userId":"12368864409965566243"}},"outputId":"cd7513bf-dcab-4249-b2df-8e899add90d7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["======================================================================\n","CREATING ZIP FILE FOR DOWNLOAD\n","======================================================================\n","\n","üì¶ Zipping all results...\n","‚úÖ Created: LR_Schedules_Complete.zip\n","üì¶ Size: 3.5 MB\n","\n","======================================================================\n","TO DOWNLOAD:\n","======================================================================\n","1. Click the folder icon üìÅ in the LEFT sidebar of Colab\n","2. You'll see the file browser\n","3. Navigate to: /content/\n","4. Find: LR_Schedules_Complete.zip\n","5. Right-click on it ‚Üí Download\n","======================================================================\n","\n","üìã What's inside the zip:\n","  ‚Ä¢ 10 JSON files (experiment results)\n","  ‚Ä¢ 4 PNG files (colorblind-friendly visualizations)\n","======================================================================\n","\n","üí° TIP: The zip file is in /content (temporary)\n","   It will disappear when you close Colab\n","   Download it NOW!\n","======================================================================\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"S9hwBRtwQV56"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyMLugbUKAizy1wEBfWQs6iI"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}